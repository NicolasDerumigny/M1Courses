\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
 
\usepackage{caption}
%\usepackage{pgfplots}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{footnote}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage[boxed,linesnumbered]{algorithm2e}
\usepackage{qcircuit}
\usepackage{enumerate}

\newtheorem{thm}{Theorem}
\newtheorem{prop}{Propriety}
\newtheorem{lemma}{Lemma}
\newtheorem{defi}{Definition}
\newtheorem{coro}{Corollary}



\setlength{\oddsidemargin}{0pt}
% Marge gauche sur pages impaires
\setlength{\evensidemargin}{0pt}
% Marge gauche sur pages paires
\setlength{\textwidth}{470pt}
% Largeur de la zone de texte 
\setlength{\topmargin}{0pt}
% Pas de marge en haut
\setlength{\headheight}{13pt}
% Haut de page
\setlength{\headsep}{10pt}
% Entre le haut de page et le texte
\setlength{\footskip}{40pt}
% Bas de page + s√©paration
\setlength{\textheight}{630pt}
% Hauteur de la zone de texte

\title{Machine Learning}
\author{Marc Sebban \& Amaury Habrard\\
\small LaHC}
\date{}


\begin{document}

\maketitle
\tableofcontents
\newpage



\section{What is Machine Learning?}
Machine Learning aims at knowing how to make algorithms that can \emph{learn} from data. They are divided in two category:
\begin{itemize}
\item Supervised learning, subdivided into 
\begin{itemize}
\item Classification: predict a yes/no answer
\item Regression: predict a continuous value, such as the price of a house
\item Ranking: output the "most relevant" data
\end{itemize}
The aim is to predict fro labelled data
\item Unsupervised learning, subdivided into 
\begin{itemize}
\item Clustering
\item Dimensionality Reduction
\end{itemize}
The aim is to find the underlying structure of unlabelled data
\end{itemize}

\paragraph{Possible Applications}
Computer Vision, Robotics, Speech Recognition, Artificial Intelligence

\paragraph{Required Skills}
\begin{itemize}
\item Convex Optimization
\item Algorithm: Asymptotic behaviour
\end{itemize}

We will mainly use SVM (Support Vector Machine), that deals with classification problems. They use the \emph{kernel trick}, which is projection of the data on a high-dimensional space (potentially infinite) where the data becomes linearly separable.

\section{Supervised learning problem}
\subsection{Notations}
\begin{itemize}
\item Let $S =\{ z_i = (\mathbf{x_i},y_i)\}^m_{i=1}$ be a set of $m$ training examples i.i.d. from an unknown joint distribution $\mathcal{D}_{\mathcal{Z}}$ over a space $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$

\item The $\mathbf{x_i}$ values ($\mathbf{x_i} \in \mathcal{X}$) are typically vectors in $\mathbb{R}^d$ whose components are usually called \emph{features}.

\item The $y$ values ($y \in \mathcal{Y}$) are drawn from a discrete set of \emph{classes/labels} (typically $\mathcal{Y}=\{-1,+1\}$ in \emph{binary classification}) or are continuous values (\emph{regression})

\item We assume that there exists a \emph{target function} $f$ such that $y=f(\mathbf{x}), \; \forall (\mathbf{x},y) \in \mathcal{X} \times \mathcal{Y}$.
\end{itemize}

\begin{defi}
A supervised learning algorithm $L$ automatically outputs from $S$ a model or a classifier (or a hypothesis) $h\in \mathcal{H}$ as close to $f$ as possible.
\end{defi}

\subsection{Curse of dimensionality - Overfitting - Underfitting}
The number of training example is very important! Sadly, as the number of features or dimension grows, the amount of data (i.e. examples necessary to learn) grows exponentially: it is the \emph{curse of dimensionality}.

To avoid this problem, we can:
\begin{itemize}
\item pre-process the data into a lower dimensional space
\item regularize the underlying optimization problem at running time
\end{itemize}

This issue is very closed to \emph{overfitting}.


\begin{defi}[Overfitting]
In statistics, \emph{overfitting} occurs when a model is excessively complex, such as having too many degrees of freedom (e.g. polynomial of high order) with respect to the amount of data available $\to$ use a \emph{regularization}.
\end{defi}

\begin{defi}[Underfitting]
\emph{Underfitting} occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data.
\end{defi}
\bigskip

To pick the best hypothesis $h^*$, we need a criterion to assess the quality of $h$. Given a non-negative loss function $\mathcal{l} : \mathcal{H} \times \mathcal{Z} \to \mathbb{R}^+$ measuring the degree of agreement between $h(\mathbf{x})$ and $y$, we can define the \emph{tree risk}.

\begin{defi}[True Risk]
The true risk $\mathcal{R}^{\ell}(h)$ (also called \emph{generalization error}) of a hypothesis $h$ with respect to a loss function $\ell$ corresponds to the expected loss suffered by $h$ over the distribution $\mathcal{D}_{\mathcal{Z}}$.

\[\mathcal{R}^{\ell}(h)=\mathbb{E}_{\mathcal{Z}\sim \mathcal{D}_{\mathcal{Z}}} \ell(h,z)\]
\end{defi}


Unfortunately, $\mathcal{R}^\ell(h)$ cannot be computed as $\mathcal{D}_\mathcal{Z}$ is unknown, so we try to minimise the \emph{empirical risk} $\hat{\mathcal{R}}^\ell$, a statistical measure of the true risk over $S$.

\begin{defi}[Empirical Risk]
Let $S = \{ z_i = (\mathbf{x_i},y_i))\}_{i=1}^m$ be a training sample. The empirical risk $\hat{\mathcal{R}}^{\ell}$ (also called empirical error) of a hypothesis $h\in \mathcal{H}$ with respect to a loss
function ` corresponds to the expected loss suffered by $h$ on $S$.
\[\hat{\mathcal{R}}_\ell(h)= \frac{1}{m} \sum_{i=1}^m \ell (h,z_i)\]
\end{defi}

\begin{defi}[0/1 loss]
The most natural loss function for binary classification is the 0/1 loss (also called classification error)
\[\ell_{0/1}(h,z)=1 \qquad\text{if $yh(x)<0$ and 0 otherwise} \]
$\mathcal{R}^{\ell_{0/1}}$ then corresponds to the proportion of correct predictions.
\end{defi}


\paragraph{Warning}
Due to the non convexity and non differentiability of the 0/1 loss, minimizing the empirical risk is NP-hard. For this reason, we use surrogate loss functions such that:
\begin{itemize}
\item \emph{Hinge loss} (used in SVM): $\ell_{hinge}(h,z) = \max (0,1-yh(x))$
\item \emph{Exponential loss} (used in boosting): $\ell_{exp}(h,z)=e^{-yh(x)}$
\item \emph{Logistic loss} (used in logistic regression): $\ell_{log}(h,z)=\ln (1+e^{-yh(x)})$
\end{itemize}

\subsection{Regularized Risk Minimization}
To prevent the algorithm from overfitting, a supervised learning problem often take the following regularized form:
\[\min_{h\in \mathcal{H}} \hat{\mathcal{R}}^\ell (h) + \lambda ||h||_p\]

Where $\lambda$ is a constant penalizing "too complex" models, and $||.||_p$ a $\ell_p$-norm over the classifier $h$.

\begin{defi}[$\ell_p$-norm]
If $\theta$ is a $d$-dimensional vector:
\[||\theta||_p = \left( \sum_{i=1}^d |\theta_i |^p \right)^{\frac{1}{p}}\]
\end{defi}

The $\ell_2$-norm is used to reduce the risk of overfitting (it decreases the large values of the model), and the $\ell_1$ also allows the induction of sparse models - i.e. with less features (example: LASSO or $\ell_1$-SVM).

\paragraph{Remark}
Increasing $\theta$ with the $\ell_1$-norm causes more and more of the parameters $\theta_j$ to be driven to zero. The gradient on the $\ell_1$-norm is constant w.r.t. the magnitude of each vector component.

\paragraph{Downside}
The $l_1$-norm is not differentiable.
\bigskip

\subsection{Bias/Variance trade-of}
There are three sources of error between $h\in \mathcal{H}$ and the target function $f\in \mathcal{F}$:
\begin{enumerate}
\item The inductive bias: nothing guarantees the equality between the target concept space $\mathcal{F}$ and the selected class of hypotheses $\mathcal{H}$, even if the learner is able to provide an optimal hypothesis $h^*$ from $\mathcal{H}$.
\item The variance: since the training set $S$ is finite and randomly drawn from $\mathcal{D}_{\mathcal{Z}}$, the learner usually does not provide the optimal hypothesis $h^*$.
\item The presence of noise: some training examples can be mislabelled. The
learner receives a training set of a "noisy" function $f_b = f + \varepsilon$.
\end{enumerate}

The Bias/Variance trade-off comes from the Mean Square Error (MSE), in statistics: 

\begin{defi}[MSE]
Let $\theta$ a theoretical parameter ($\mathcal{R}(h)$ in our case) and $\hat{\theta}$ an estimate of $\theta$ ($\hat{\mathcal{R}}(h)$ in our case). Let $B=\mathbb{E}(\theta) - \theta$ be the bias of $\hat{\theta}$ w.r.t. $\theta$. The MSE assesses the quality of $\theta$ in terms of its variation and unbiasedness. It is the expected value of the square loss between $\hat{\theta}$ and $\theta$.
\begin{align*}
MSE & = \mathbb{E}_z [(\hat{\theta}-\theta)^2]\\
&= \mathbb{E}_z[(\hat{\theta}-\mathbb{E}(\hat{\theta}) + \mathbb{E}(\hat{\theta})- \theta)^2]\\
&= \mathbb{E}_z[(\hat{\theta}-\mathbb{E}(\hat{\theta}) + B)^2]\\
& = \mathbb{V}(\hat{\theta}) + B^2
\end{align*}
\end{defi}


\subsection{Statistical learning theory}
\begin{defi}[Empirical Risk Minimization]
The ERM principle rests on the fact that if $h$ works well on the training set $S$ it might also work well on new examples.
\end{defi}

\begin{defi}[Probably Approximately Correct (PAC) Condition][Valiant 1984]
The ERM principle is valid if the true risk of the hypothesis $h \in \mathcal{H}$ induced from $S$ is closed to the true risk of the optimal hypothesis $h^* \in \mathcal{H}$
\begin{align*}
h &= \arg \min_{h_i \in \mathcal{H}} \hat{\mathcal{R}}(h_i)\\
h^* &= \arg \min_{h_i\in \mathcal{H}} \mathcal{R}(h_i)
\end{align*}

Condition of validity of the ERM principle:
\[\forall \mathcal{D}_\mathcal{Z}, \forall \gamma \geq 1, \forall \delta \leq 1, \mathbb{P}(|\mathcal{R}(h)=\mathcal{R}(h^*)|\geq \gamma) \leq \delta\]
\end{defi}

\begin{defi}[Bayesian error]
\label{def:BayErr}
The bayesian error $\epsilon^*$ is the lowest possible error rate (or irreducible error) for any hypothesis $h$.
\[\epsilon^8 = \int_{x\in R_i \text{ s.t. }y\neq C_i} \mathbb{P}(C_i|x)\mathbb{P}(x)dx\]
where $x$ is an instance, $y$ its corresponding label, $R_i$ is the area/region that a classifier function $h$ classifies as $C_i$.
\end{defi}


\paragraph{Remark}
In many application, $\epsilon^* >0$, and as $S$ is finite, selecting the optimal $h$ does not imply getting the optimal hypothesis $h^*$.
\bigskip


The law of large numbers prompts us to increase the size of the learning set and to search for the minimal size m that allows us to fulfil the PAC
condition:
\[\forall \mathcal{D}_{\mathcal{Z}}, \forall \gamma \geq 0, \forall \delta \leq 1, \exists m \text{ s.t. } \mathbb{P}(|\mathcal{R}(h_m)-\mathcal{R}(h^*)|\geq \gamma) \leq \delta\]
Where $h_m$ is the hypothesis learnt from a training set of size $m$.


\paragraph{Question} What is the conditions on the \emph{minimum number of required examples} for the empirical risk and the true risk of the induced hypothesis $h$ to converge towards the true risk of $h^*$ ?\\
We will differentiate $|\mathcal{H}|$ finite and $|\mathcal{H}|$ infinite. 


\subsubsection{When $|\mathcal{H}|$ is finite}


%TODO : miss to page 44 to 53
\subsubsection{When $|\mathcal{H}|$ is infinite}


\paragraph{Remarks}
On the VC theory.
\begin{itemize}
\item The only property that matters is the \emph{size of the hypothesis space} and not \emph{how the algorithm searches the space}

\item Therefore, the VC theory is meaningful when the learning algorithm performs minimization of $\hat{\textsc{R}}(h)$ in the \emph{full hypothesis space}.
\item It is \emph{useless for local algorithms}, like the $k$-NM which has an infinite $d_{\mathcal{H}}$.

\item Two analytical frameworks to take into account the algorithm $L$ to derive generalization bounds: \emph{Uniform stability} and {Algorithmic robustness}. The goal is to bound:
\[ \mathbb{P} (|\mathcal{R}(L, h_S ) - \hat{\mathcal{R}}(L, h_S)| \geq \gamma) \]
which differs from what we studied before:

\[ \mathbb{P} ( \sup_{h\in \mathcal{H}}|\mathcal{R}(h) - \hat{\mathcal{R}}(h)| \geq \gamma) \]
\end{itemize}

\subsection{Uniform stability}
We only focus here on Uniform Stability.
Variance versus Stability
Statistical learning theory prompts us to reduce the variance
without altering the bias.
Having a low variance is equivalent to having high stability.
How to relate the generalization error Rh to the stability of an
algorithm L which induces h?
Intuitively, an algorithm L is said stable if it is robust to small changes in
the training sample, i.e., the variation in its output h is small.

Given a training set $S$ of size $m$, we build $\forall i =1 ... m$:
\begin{itemize}
\item $S^{\setminus i}= \{ z_1,...,z_{i-1}, z_{i+1},...,z_m\}$ by removing the $i$-th element of $S$.
\item $S^{i}= \{ z_1,...,z_{i-1},z_i',z_{i+1},...,z_m\}$ by replacing the $i$-th element by $z_i'$ drawn i.i.d. from $\mathcal{D}_{\mathcal{Z}}$.
\end{itemize}


\begin{defi}[Uniform stability][Bousquet and Elisseef][2002]
An algorithm $L$ has uniform stability $\frac{\beta}{m}$ with respect to a loss function $\ell$ if the following holds:
\[\forall S, \forall i \in \{1,...,m\}, \sup_{z} |\ell(h_S,z)-\ell(h_{S^{\setminus i}},z)|\leq \frac{\beta}{m}\]
Where $\beta$ is a positive constant, $h_S$ and $h_{S^{\setminus i}}$ are the hypothesis learned by $L$ from $S$ and $S^{\setminus i}$ respectively.
\end{defi}

\begin{prop}[Generalisation bound using uniform stability]
Let $S$ be a training sample of size $m$ and $\delta>0$. For any algorithm $L$ with uniform stability $\frac{\beta}{m}$ with respect to a loss function $\ell$ bounded by $M$, with probability $1-\delta$, we have:
\[\mathcal{R}_{h_S}\leq \hat{\mathcal{R}}_{h_S}+\frac{2\beta}{m}+(4\beta+M)\sqrt{ \frac{ \ln \frac{1}{\delta}}{2m} }\]
\end{prop}

\begin{thm}[Kearns and Ron][1999]
An algorithm $L$ having an hypothesis space of finite VC-dimension is stable in the sens that its stability is bounded by ots VC-dimension.
\end{thm}

\begin{coro}
Using the stability as a complexity measure does not give worse bounds than using the VC-dimension.
\end{coro}

\begin{proof}Of Generalisation Bound.\\
The proof is based on McDiarmid's Theorem (1989).
\begin{thm}[][McDiarmid][1989]
Let $S$ and $S^i$ defined as above, let $F : \mathcal{Z}^m \to \mathbb{R}$ be a function for which
there exists constants $c_i (i = 1, ...,m)$ such that
\[\sup_{s\in \mathcal{Z}^m,z_i'\in \mathcal{Z}}|F(S)-F(S^i)|\leq c_i\]

then

\[P_S(F(S)-\mathbb{E}[F(S)]\geq \gamma) \leq e^{-2\gamma^2/\sum_{i=1}^{m} c_i^2}\]

\end{thm}
Let's show that $F(S) = \mathcal{R}_{h_S}-\hat{\mathcal{R}}_{h_S}$ (for the sake of simplicity
$F(S) = \mathcal{R} - \hat{\mathcal{R}}$) and $F(S^i ) = \mathcal{R}^i - \hat{\mathcal{R}}^i$ satisfy the previous condition.

See more details in the slides
%TODO : copy the proof :(
\end{proof}

%warning : second mistake in the slides : p63 its P_S[R-^R>\gamma + 2\beta/m] \leq ...


\paragraph{How to find the stability constant?}
When the learning problem takes the following form:
\[\min_{h\in \mathcal{H}} \frac{1}{m} \sum_{i} \ell (h,z_i) + \lambda||h||^2_{\mathcal{F}}\]
we can show [Bousquet and Elisseef 2002] that the stability constant is defined as follows:
\[\beta \leq \frac{\sigma^2}{2\lambda}\]
where $\sigma$ comes from the $\sigma$ admissibility of $\ell(h,z)=c(h(x),y)$ where $c$ is the associated cost function.

\begin{defi}[$\sigma$-admissibility]
A loss function $\ell$ is $\sigma$-admissible if the associated cost function $c(h(x),y)$ is convex with respect to its first argument and the following condition holds:
\[\forall y_1,y_2 \in \mathcal{Z}, \forall y' \in \mathcal{Y}, |c(y_1,y')-c(y_2,y')|\leq \sigma |y_1-y_2|\]
\end{defi}
\paragraph{Examples: }
See tutorial.

\subsection{Model Selection}
We saw that there is pften a trade-off between the bias and variance:  it is important not to choose a hypothesis that is either too simple (underfitting) or too complex (overfitting). Model selection algorithms provide a method that automatically makes the trade-off between bias and variance.

\paragraph{Examples}
\begin{enumerate}
\item Linear regression: What degree of the polynomial do you need to select?
\item $k$ nearest-neighbours: What is the right number $k$ of neighbours?
\end{enumerate}


\subsubsection*{Model Selection}
How to choose the best hypothesis in $M$ ($M=\{h_1,h_2,...)$
\begin{itemize}
 \item Bad Idea: choose the onbe with the lowest training error $\hat{\mathcal{R}}(h)$ (risk of overfitting)
 \item Good Idea: Hold-out $k$ cross-validation 
 \end{itemize} 

\begin{algorithm}
\KwIn{A learning algorithm $L$ and a learning set $S$ of $m$ examples}
\KwOut{An estimate $\hat{\mathcal{R}}'(h)$}
Split $S$ randomly in $k$ subsets $S_1,...,S_k$ (if $k=m$, leave-one-out CV)\;
\For{i=1 to k}{
	Run $L$ on $S-S_i$ and induce classifier $h_i$\;
}
Deduce the estimate $\hat{\mathcal{R}}'(h)$ of the true risk s.t. $\hat{\mathcal{R}}'(h) =
\frac{1}{k} \sum_{i=1}^{k} \hat{\mathcal{R}}(h_i)$ where $\hat{\mathcal{R}}(h_i)$ is the error of $h_i$ on $S_i$\;
\caption{Hold-out $k$ cross-validation algorithm}
\end{algorithm}

\section{The Regression Problem}
\subsection{Introduction}
\paragraph{The Regression Problem}
How to optimize the parameter of the hyperplane
\[h_\theta (x) = \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_0\]
which fits the best the following set of training example $S$ ?

\paragraph{Notations}
\begin{itemize}
\item $m$ is the number of training examples
\item $x\in \mathbb{R}^n$ is the input feature vector of $n$ variables.
\item $y$ is the output variable/target
\item $(x,y)$ is the training example
\item $(x^{(i)},y^{(i)})$ is the $i$-th training example
\item $h$ is the hypothesis that maps from input $x$ to output $y$.
\item $h(x)$ is of the linear form: $h(x)=\theta_0 + \theta_1x_1 + ... + \theta_nx_n$. For conciseness, we define $x_0=1$ so that 
\[ h(x) = h_{\theta}(x) = \sum_{i=0}^n \theta^ix_i = \theta^T x \]
Where $\theta=(\theta_0,...,\theta_n)\in \mathbb{R}^{n+1}$.
\end{itemize}


We aims at choosing the parameters $\theta$ so that the hypothesis $h$ will make accurate predictions.

\begin{defi}[Non regularized Least Squares Problem]
\[\min_{\theta} J(\theta) = \min_{\theta} \frac{1}{2m}\sum_{i=1}^m \Big( h_\theta(x^{(i)}) - y^{(i)}\Big)^2\]
\end{defi}

\subsection{Learning Algorithms}
There exist three main solutions for minimizing $J(\theta)$:
\begin{itemize}
\item Batch Gradient Descent
\item Stochastic Gradient Descent
\item Closed-form solution
\end{itemize}

\paragraph{Basic Idea}
Let us assume that $J(\theta)$ is differentiable:
\begin{itemize}
\item Start with some initialization of $\theta$ (e.g. $\vec{\theta} =0$ or some randomly
chosen vector.)
\item Update$\theta$'s values so that to reduce $J(\theta)$) (by computing partial
derivatives of $J(\theta)$ w.r.t. $\theta$).
\item Repeat the process till convergence to the minimum of $J(\theta)$
\end{itemize}

\paragraph{Update Rule}
%TODO


\paragraph{Remark}
Note that with a slightly different initial starting point, you can end up to a completely different optimum. Fortunately,$J(\theta)$ is a \emph{quadratic function} with only \emph{one global optimum}.

\paragraph{Update of the $i^{th}$ parameter of $\theta$}
Cf slides for calculus
We get the following update rule:
\[ \theta_i := \theta_i - \alpha(h_\theta(x)-y)x_i\]

\begin{algorithm}
Initialized $\vec{\theta}$\;
\Repeat{convergence (i.e. "stabilisation" of $J(\theta)$)}{
	$\forall \theta_i$ of $\theta$, $\theta_i := \theta_i - \alpha\frac{1}{m}\sum_{j=1}^m(h_\theta(x)-y)x_i$
}
\caption{Batch Gradient Descending Algorithm}
\end{algorithm}




\paragraph{Remark}
If $m$ is huge, then the batch gradient descent will perform at each step a huge summation, therefore we need another algorithm.

\begin{algorithm}
Initialized $\vec{\theta}$\;
\Repeat{convergence (i.e. "stabilisation" of $J(\theta)$)}{\For{j=1 to m}{
	$\forall \theta_i$ of $\theta$, $\theta_i := \theta_i - \alpha(h_\theta(x^{(j)})-y^{(j)})x_i^{(j)}$
	}
}
\caption{Stochastic Descending Algorithm}
\end{algorithm}

\paragraph{Remarks}
The Stochastic Gradient Descent is much faster, and start each update of $\theta$ with the first training example, then the second, ... . However, it won't converge exactly to the global minimum, even though it tends to wander around some regions close to the global minimum.

\paragraph{Influence of the learning rate $\alpha$}
For some specific examples, $J(\theta)$ may increase. This may occur in some situations where $\theta$ is large (``zigzag" effect). To prevent the parameters $\theta$'s from oscillating around the global minimum we can take a smaller learning rate.

In most implementations, the learning rate is held constant. However, if you want to converge to ``a minimum" you can slowly decrease $\alpha$ over time, such that at iteration i we get:
\[\alpha_i = \frac{C_1}{i+C_2}\]
which means you're guaranteed to converge ``somewhere".


\paragraph{Mini-batch gradient descent}
A compromise between a batch gradient descent and a stochastic gradient descent, is to compute the gradient only on a (randomly selected) bunch of training examples (called a ``\emph{mini-batch}") at each step.

It may result in smoother convergence, as the gradient computed at each step uses more training examples than in the stochastic setting.



\paragraph{Closed-form Solution}
We use massively the trace and its proprieties (Cf slides for details).
The optimal solution, $\nabla_\theta J(\theta)\overset{\text{set}}{=} \vec{0}$ boils down to
\[\theta = (X^TX)^{-1}X^Ty\]
Where
\[X= \begin{pmatrix}
(x^{(1)})^T \\
(x^{(2)})^T \\
\vdots\\
(x^{(n)})^T\\
\end{pmatrix}
\]

\paragraph{Issues:}
$(X^TX)^{-1}$ can be not invertible:
\begin{itemize}
\item Redundant features (linearly dependent) $\to$ perform a PCA (Principal Component Analysis)
\item Too many features $\to$ delete some irrelevant features (feature selection algorithm)
\end{itemize}

\paragraph{Remarks}
For complexity reasons, we prefer to use the gradient descent rather than the normal equation when $n$ is height.

\subsection{Objective function in linear regression}
The objective function used is
\[ \min_{\theta}  \frac{1}{2m} \sum_{i=1}^m \Big(h_\theta (x^{(i)}) - y^{(i)} \Big)^2\]

\begin{defi}[Likelihood]
Let $x_1, x_2, ... x_m$ be a random set of $m$ i.i.d. observations, coming from an unknown

\end{defi}

So we use the quadratic loss because it is equivalent to maximising the likelihood assuming that there are i.i.d gaussian errors on the data

%TODO : missing slides here

%polynomial regression : equal to linear

%Ridge regression

%LASSO


\subsection{Limitations of Linear Regression}
Until now, we assumed that $y\in \mathbb{R}$ (continuous variable). Let us now assume that $y \in \{0,1\}$ (discrete variable).

\[\text{predicting }y \simeq \text{ classification task}\]

Sometimes, it will work, but it is actually a pretty bad idea.
\bigskip	

\paragraph{} Let us assume that $y\in \{0,1\}$ and $h_{\theta} \in [0,1]$. Let us define $h_\theta(x)$ as follows:
\[h_{\theta}(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\]
where $g(z)=\frac{1}{1+e^{-z}}$ is the \emph{sigmoid (or logistic)} function.
\bigskip

Assuming we aims at learning $h_\theta(x)$ such that $\mathbb{P}(y=1|x,\theta)=h_\theta(x)$ and $\mathbb{P}(y=0|x,\theta)=1-h_\theta(x)$ we deduce that the likelihood is:
\[L(\theta) = \mathbb{P}(\vec{y}|x;\theta) = \prod_{i} \mathbb{P}(y^{(i)}|x^{(i)},\theta) = \prod_{i}h_\theta ( x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\]


As done before, it is easier to maximize the log of the likelihood $\ell(\theta)=\ln L(\theta)$, which leads to the same solution as the least square regression. However, it is not the same algorithm as the objective function is not the same.
\bigskip

There exists a faster method than the gradient algorithm: \emph{Newton's method}.

The update rule is then:
\[\theta^{t+1}=\theta^{t}-\dfrac{f(\theta^t}{f'(\theta^t)}\]

Applying to the log-likelihood, we get:
\[\theta^{t+1}=\theta^{t}-H^{-1}\nabla_\theta \ell\]

Where $H$ is the Hessian matrix, $\nabla_\theta \ell$ is the gradient

\paragraph{Advantages and disadvantages}
The Newton's method is faster in term of number of steps, but at each iteration, we need to invert the $n \times n$ Hessian matrix, where $n$ is the number of features, which can be very costly.


\section{Sparsity in Convex Optimisation for Supervised Machine Learning}


\paragraph{Reminder}
With $S=\{z_i = (x_i,y_i)\}_{i=1}^m$ the set of trainings (i.i.d.) from an unknown joint distribution $\mathcal{D}_\mathcal{Z}$ over a space $\mathcal{Z}=\mathcal{X}\times  \mathcal{Y}$
\[\min_{h} \sum_{i=1}^m \ell (y_i,h(x_i))+\lambda||h||\]
This can be rewritten as constrained problem:

\[\min_{h} \sum_{i=1}^m \ell (y_i,h(x_i))\qquad \text{such that } ||h||<c\]
\bigskip

We consider that the set $S$ is composed of $m$ examples in $\mathbb{R}^d$.
\[
S = \begin{pmatrix}
x_1\\
.\\
.\\
.\\
x_i\\
.\\
.\\
.\\
x_m\\
\end{pmatrix}
=
\begin{pmatrix}
x^1& ... & x^j & ... & x^d
\\\end{pmatrix}
\]


This set can be reduced:
\begin{itemize}
\item in columns $\to$ deletion of features, useful when $d$ is large compared to $m$
\item in rows $\to$ deletion of examples (useful in $\ell_1$-SVM, CNN).
\item in rank (e.g. PCA, LSA) $\to$ find the embedding space
\end{itemize}

Reducing can also:
\begin{itemize}
\item Prevent the algorithm from overfitting (curse of dimensionality) $\to$ Occam's razor principle
\item Computational efficiency
\item Interpretability: $\to$ understand the unerlying phenomenon
\end{itemize}


There are three categories of methods:
\begin{itemize}
\item \emph{Filter approach} Use a criterion to filter, then learn after the treatment
\item \emph{Wrapper approach} Heuristic search of subset of variable, done with respect to the learning algorithm performance
\item \emph{Embedded approach} Use of sparsity-inducing norms :
\begin{itemize}
\item Feature selection is part of the learning algorithm
\item All features processed during learning (ex: LASSO)
\end{itemize}
\end{itemize}

\paragraph{Note}
We prefer to use the $\ell_0$ norm rather than any other to induce sparsity. As usual, we will use the $\ell_1$-norm, as it is a good compromise between having a convex relation ($\leq 1$) and having a sparse solution ($\geq 1$).

\subsection{$\ell_1$-norm Regularization}
$\ell_1$-norm Regularization performs regularization as well as feature selection.

$\lambda$ can be seen as the range of value for what the value of the parameter is zero.


\subsection{Optimization methods}
We can use different tricks:
\[\mathbf{w}_j = \mathbf{w}_j^+ + \mathbf{w}_j^-\]

Or noticing that $||\mathbf{w}||_1=\min_{\eta \geq 0}\frac{1}{2} \sum_{j=1}^d \dfrac{\mathbf{w}_j^2}{\eta_j}+\eta_j $

We can also use group lasso, if we have a partition of data to prioritise, and use the $\ell_1/\ell_2$-norm $ = \sum_{g\in \mathcal{G}} || \mathbf{w}_g||_2$ where ${\mathcal{G}_k}_{k=1}^K$ forms a partition of $\{1,...,d\}$.


\section{$k$-nearest neighbour}
\paragraph{Recall}
The bayesian error is defined in definition \ref{def:BayErr}.

\paragraph{Bayesian Classifier}
The Bayesian classifier predicts the optimal class $y^*\in \mathcal{Y}$ given an example $\mathbf{x} \in \mathcal{X}$ by applying the Maximum A Posteriori (MAP) decision rule:
\begin{align*}
\forall y_j \in \mathcal{Y}, p(y_i|\mathbf{x}) = \dfrac{p(\mathbf{x}|y_j).p(y_j)}{p(\mathbf{x})}\\
y^* (\mathbf{x} = \arg \max_c p(y_c | \mathbf{x}) \\
\end{align*}
that corresponds to $y^* = \arg \max_c p(\mathbf{x}|y_c).p(y_c)$, which is optimal from a probabilistic point of view.
\bigskip

However, this supposes to know $p(y_j)$ and $p(\mathbf{x}|y_j)$, which is not the case. We can estimate $p(y_j)$ by $\hat{p}(y_j)=\dfrac{|S_j|}{|S|}$, and $p(\mathbf{x}|y_j)$ by either a parametric method, that assume it follows a given statistical distribution (and we estimate it), or a non-parametric method, that estimates $p(\mathbf{x}|y_i)$ is locally estimated around $\mathbf{x}$.

An example of non-parametric method is $p(\mathbf{x}) \simeq \dfrac{k}{nV} = \hat{p}(\mathbf{x})$ (cf slides).








\begin{thm}
Let us denote by $\hat{p}_n(\mathbf{x})=\frac{k_n}{nV_n}$ the estimate of $p(\mathbf{x})$ from a training sample $S$ of size $n$. When $n$ increases, $\hat{p}(\mathbf{x})$ converges to $p(\mathbf{x})$ if the following three conditions are fulfilled:
\begin{itemize}
\item $\lim_{n\to\infty} V_n = 0$
\item $\lim_{n\to\infty} k_n = \infty$
\item $\lim_{n\to\infty} \dfrac{k_n}{n} =0$
\end{itemize}
\end{thm}

The $k$-Nearest Neighbour satisfy the previous theorem. It fixes a number $k_n$ of examples, and adapts a volume $V_n$ (e.g. an hypersphere centred at $\mathbf{x}$) such that $k_n$ examples are in the volume.
\bigskip

Using $k$-NN as a classifier, we get that $h(\mathbf{x})= \arg \max_{j} = \dfrac{k_j}{k}$

\begin{algorithm}
\KwIn{$x,S,d$}
\KwOut{class of $x$}
\For{$(\mathbf{x}',y')\in S$}{
Compute the distance $d(\mathbf{x}',\mathbf{x})$\;
}
Sort the $n$ distances by increasing order\;
Count the number of occurrences of each class $y_i$ among the $k$ nearest neighbours\;
Assign to $\mathbf{x}$ the most frequent class \;
\end{algorithm}

\begin{prop}[Convergence of the 1-nearest neighbour]
Let $\mathbf{x}'$ be the nearest neighbour of $\mathbf{x}$, then 
\[\lim n\to \infty p(d(x,x') > \epsilon) = ), \forall \epsilon 0\]
and thus in $n \to \infty, p(y_j|\mathbf{x}')\simeq p(y_j | \mathbf{x}).$
\end{prop}

\begin{thm}
The generalization error $\epsilon_{1NN}$ of the $1$-nearest neighbour rule is bounded by twice the (optimal) bayesian error $\epsilon^*$
\[e_{1NN}\leq 2\epsilon^*\]
\begin{coro}
Half the informations about the true class of an example $\mathbf{x}$ is contained in its nearest neighbour $\mathbf{x}'$
\end{coro}
\end{thm}

\paragraph{Issue}
This is only an asymptotic result $\to$ it needs a large number of training examples, and thus implies a large space/time complexity.

To cope with that, we can reduce the size of $S$ by keeping some examples only, and simplify the calculation of the nearest neighbour.


\subsection{Data reduction techniques}
It can be seen as a pretreatment on the set, that remove from $S$ the outliers and the examples of the bayesian error region.

\begin{algorithm}
\KwIn{$S$}
\KwOut{$S_{cleaned}$}
Split randomly $S$ into two subset $S_1$ and $S_2$\;
\While{no stabilization of $S_1$ and $S_2$}{
	Classify $S_1$ with $S_2$ using 1-NN rule\;
	Remove from $S_1$ the misclassified instances\;
	Classify $S_2$ with the new set $S_1$ using the $1-NN$ rule\;
	Remove from $S_2$ the misclassified instances\;
}
$S_{cleaned}=S_1\cup S_2$\;
\end{algorithm}

\begin{algorithm}
\KwIn{$S$}
\KwOut{STORAGE}
STORAGE $\leftarrow \emptyset$\;
DUSTBIN $\leftarrow \emptyset$\;
Draw randomly a training example from $S$ and put it in STORAGE\;
\While{no stabilization of STORAGE}{
	\For{earch $\mathbf{x}_i \in S$}{
		\eIf{$\mathbf{x}_i$ is correctly classified with STORAGE using the 1-NN rule}
		{
			DUSTBIN $\leftarrow \mathbf{x}_i$\\
		}{
			STORAGE $\leftarrow \mathbf{x}_i$\\
		}
	}
} 
\end{algorithm}

\subsection{Speeding-up the nearest-neighbour calculation}
Most of the approach are based on the \emph{triangle inequality property}.



\end{document}